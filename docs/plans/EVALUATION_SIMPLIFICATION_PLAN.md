# Evaluation Module Simplification Plan

**Date**: January 8, 2026  
**Status**: Proposed  
**Goal**: Reduce abstraction complexity while maintaining functionality

---

## ðŸŽ¯ Executive Summary

The evaluation module suffers from **over-abstraction**:
- Too many base classes and inheritance hierarchies
- Complex configuration system
- Hard for users to create custom criteria
- Similar abstraction problems as the LLM SDK issue

**Solution**: Apply the **Minimal Wrapper Pattern** - simplify to functional approach with composition.

---

## ðŸ“Š Current Architecture Analysis

### Current Structure (Complex)

```
agentflow/evaluation/
â”œâ”€â”€ evaluator.py (560 lines)          # Main orchestrator
â”œâ”€â”€ eval_config.py (286 lines)        # Complex config hierarchy
â”œâ”€â”€ eval_result.py (351 lines)        # Complex result hierarchy
â”œâ”€â”€ eval_set.py (356 lines)           # Test case models
â”œâ”€â”€ testing.py (327 lines)            # Testing helpers
â”œâ”€â”€ criteria/                          # 5 files, complex inheritance
â”‚   â”œâ”€â”€ base.py                       # BaseCriterion, SyncCriterion, CompositeCriterion, WeightedCriterion
â”‚   â”œâ”€â”€ response.py                   # ResponseMatchCriterion, ExactMatchCriterion, ContainsKeywordsCriterion
â”‚   â”œâ”€â”€ trajectory.py                 # TrajectoryMatchCriterion, ToolNameMatchCriterion
â”‚   â”œâ”€â”€ llm_judge.py                  # LLMJudgeCriterion, RubricBasedCriterion
â”‚   â””â”€â”€ advanced.py                   # FactualAccuracyCriterion, HallucinationCriterion, SafetyCriterion
â”œâ”€â”€ collectors/                        # Trajectory collection
â”‚   â””â”€â”€ trajectory_collector.py
â”œâ”€â”€ reporters/                         # 3 reporter types
â”‚   â”œâ”€â”€ console.py
â”‚   â”œâ”€â”€ html.py
â”‚   â””â”€â”€ json.py
â””â”€â”€ simulators/                        # User simulation
    â””â”€â”€ user_simulator.py
```

**Total**: ~3,000+ lines of code

### Problems Identified

#### 1. **Over-abstraction in Criteria**

```python
# Current: Complex inheritance
class BaseCriterion(ABC):
    @abstractmethod
    async def evaluate(...) -> CriterionResult: pass

class SyncCriterion(BaseCriterion):
    @abstractmethod
    def evaluate_sync(...) -> CriterionResult: pass
    async def evaluate(...): return self.evaluate_sync(...)

class CompositeCriterion(BaseCriterion):
    def __init__(self, criteria: list[BaseCriterion], require_all: bool): ...

class WeightedCriterion(BaseCriterion):
    def __init__(self, criteria_weights: list[tuple[BaseCriterion, float]]): ...

# Then each actual criterion extends one of these:
class TrajectoryMatchCriterion(SyncCriterion): ...
class ResponseMatchCriterion(SyncCriterion): ...
class LLMJudgeCriterion(BaseCriterion): ...
```

**Issue**: Users who want custom criteria face steep learning curve.

#### 2. **Complex Configuration System**

```python
# Current: Nested configuration objects
class Rubric(BaseModel):
    rubric_id: str
    content: str
    weight: float = 1.0

class CriterionConfig(BaseModel):
    threshold: float = 0.8
    match_type: MatchType = MatchType.EXACT
    judge_model: str = "gpt-4o-mini"
    num_samples: int = 3
    rubrics: list[Rubric] = Field(default_factory=list)
    check_args: bool = True
    enabled: bool = True

class EvalConfig(BaseModel):
    criteria: dict[str, CriterionConfig] = Field(default_factory=dict)
    user_simulator_config: UserSimulatorConfig | None = None
    parallel: bool = False
    max_concurrency: int = 4
    # ... more fields
```

**Issue**: Too much configuration ceremony for simple use cases.

#### 3. **Complex Result Hierarchy**

```python
# Current: Multiple result classes
class CriterionResult(BaseModel):
    criterion: str
    passed: bool
    score: float
    threshold: float
    details: dict[str, Any] = Field(default_factory=dict)
    error: str | None = None

class EvalCaseResult(BaseModel):
    eval_id: str
    name: str | None = None
    passed: bool
    criterion_results: list[CriterionResult]
    actual_trajectory: list[TrajectoryStep]
    actual_tool_calls: list[ToolCall]
    actual_response: str
    duration_seconds: float
    error: str | None = None

class EvalReport(BaseModel):
    eval_set_id: str
    results: list[EvalCaseResult]
    summary: EvalSummary
    # ... more fields
```

**Issue**: Too many layers to navigate for simple results.

---

## ðŸ’¡ Proposed Solution: Functional + Composition Approach

### Core Philosophy

1. **Functions over Classes** - Most evaluations are just functions
2. **Composition over Inheritance** - Build complex logic from simple functions
3. **Escape Hatches** - Allow direct access for power users
4. **Sensible Defaults** - Make common cases trivial

### Proposed Architecture

```
agentflow/evaluation/
â”œâ”€â”€ evaluate.py (200 lines)           # Main evaluation function + helpers
â”œâ”€â”€ criteria.py (300 lines)           # All criteria as simple functions
â”œâ”€â”€ models.py (200 lines)             # Simple data models (EvalSet, Result)
â”œâ”€â”€ collectors.py (150 lines)         # Trajectory collection (keep as-is, it's good)
â”œâ”€â”€ reporters.py (150 lines)          # Simplified reporters
â””â”€â”€ simulators.py (200 lines)         # User simulation (keep as-is, it's good)
```

**Total**: ~1,200 lines (60% reduction!)

---

## ðŸ”§ Implementation Details

### 1. Simplify Criteria: Functions over Classes

**Before (Current)**:
```python
class TrajectoryMatchCriterion(SyncCriterion):
    name = "trajectory_match"
    description = "Matches tool trajectory"
    
    def __init__(self, config: CriterionConfig | None = None):
        super().__init__(config)
    
    def evaluate_sync(
        self,
        actual: TrajectoryCollector,
        expected: EvalCase,
    ) -> CriterionResult:
        # 50+ lines of matching logic
        ...
        return CriterionResult.success(...)
```

**After (Proposed)**:
```python
# criteria.py - Simple functions

def trajectory_match(
    actual_trajectory: list[ToolCall],
    expected_trajectory: list[ToolCall],
    match_type: str = "exact",
    check_args: bool = True,
) -> dict:
    """Check if actual trajectory matches expected.
    
    Args:
        actual_trajectory: List of actual tool calls
        expected_trajectory: List of expected tool calls
        match_type: "exact", "in_order", or "any_order"
        check_args: Whether to check tool arguments
    
    Returns:
        {
            "passed": bool,
            "score": float,
            "details": {...}
        }
    """
    if match_type == "exact":
        return _exact_match(actual_trajectory, expected_trajectory, check_args)
    elif match_type == "in_order":
        return _in_order_match(actual_trajectory, expected_trajectory, check_args)
    else:
        return _any_order_match(actual_trajectory, expected_trajectory, check_args)


def response_similarity(
    actual_response: str,
    expected_response: str,
    method: str = "rouge1",
) -> dict:
    """Compute similarity between responses.
    
    Args:
        actual_response: Actual agent response
        expected_response: Expected response
        method: "rouge1", "rouge2", "bleu", or "exact"
    
    Returns:
        {
            "passed": bool,
            "score": float,
            "details": {...}
        }
    """
    if method == "exact":
        score = 1.0 if actual_response == expected_response else 0.0
    elif method == "rouge1":
        score = _compute_rouge1(actual_response, expected_response)
    # ... other methods
    
    return {
        "passed": score >= 0.8,  # Can be overridden
        "score": score,
        "details": {"method": method},
    }


async def llm_judge(
    actual_response: str,
    expected_response: str,
    question: str,
    model: str = "gpt-4o-mini",
    num_samples: int = 1,
) -> dict:
    """Use LLM to judge response quality.
    
    Args:
        actual_response: Actual agent response
        expected_response: Expected response (reference)
        question: Original user question
        model: Model to use for judging
        num_samples: Number of samples for majority vote
    
    Returns:
        {
            "passed": bool,
            "score": float,
            "details": {...}
        }
    """
    # Simple LLM judge implementation
    from openai import AsyncOpenAI
    
    client = AsyncOpenAI()
    
    prompt = f"""Rate how well the actual response answers the question compared to the reference.
Question: {question}
Reference Answer: {expected_response}
Actual Answer: {actual_response}

Rate from 0.0 to 1.0 where 1.0 means perfect match in quality."""
    
    scores = []
    for _ in range(num_samples):
        response = await client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
        )
        score = _extract_score(response.choices[0].message.content)
        scores.append(score)
    
    avg_score = sum(scores) / len(scores)
    
    return {
        "passed": avg_score >= 0.8,
        "score": avg_score,
        "details": {"samples": scores, "model": model},
    }


# Composition helpers
def combine_criteria(
    criteria_results: list[dict],
    logic: str = "all",  # "all" or "any"
) -> dict:
    """Combine multiple criterion results.
    
    Args:
        criteria_results: List of criterion result dicts
        logic: "all" (AND) or "any" (OR)
    
    Returns:
        Combined result dict
    """
    if logic == "all":
        passed = all(r["passed"] for r in criteria_results)
        score = min(r["score"] for r in criteria_results)
    else:
        passed = any(r["passed"] for r in criteria_results)
        score = max(r["score"] for r in criteria_results)
    
    return {
        "passed": passed,
        "score": score,
        "details": {"sub_results": criteria_results, "logic": logic},
    }


def weighted_criteria(
    criteria_results: list[tuple[dict, float]],  # (result, weight)
) -> dict:
    """Compute weighted average of criteria.
    
    Args:
        criteria_results: List of (result, weight) tuples
    
    Returns:
        Combined result dict
    """
    total_weight = sum(w for _, w in criteria_results)
    weighted_sum = sum(r["score"] * w for r, w in criteria_results)
    score = weighted_sum / total_weight
    
    return {
        "passed": score >= 0.8,
        "score": score,
        "details": {
            "sub_results": [
                {"score": r["score"], "weight": w}
                for r, w in criteria_results
            ]
        },
    }
```

**Benefits**:
- âœ… No inheritance complexity
- âœ… Easy to understand and extend
- âœ… Compose functions to build complex logic
- âœ… Can still use classes if needed (escape hatch)

### 2. Simplify Configuration: Dict-based

**Before (Current)**:
```python
config = EvalConfig(
    criteria={
        "trajectory_match": CriterionConfig.trajectory(
            threshold=1.0,
            match_type=MatchType.EXACT,
            check_args=True,
        ),
        "response_match": CriterionConfig.response_match(
            threshold=0.8,
        ),
    }
)
```

**After (Proposed)**:
```python
# Simple dict-based configuration
config = {
    "criteria": [
        {
            "name": "trajectory_match",
            "threshold": 1.0,
            "match_type": "exact",
            "check_args": True,
        },
        {
            "name": "response_similarity",
            "threshold": 0.8,
            "method": "rouge1",
        },
    ],
    "parallel": False,
    "verbose": True,
}

# Or even simpler for common cases:
config = "default"  # Uses built-in defaults
# or
config = "strict"   # Stricter thresholds
```

**Benefits**:
- âœ… Less ceremony
- âœ… Easy to read/write JSON
- âœ… Still type-safe with validation

### 3. Simplify Evaluator: Single Function

**Before (Current)**:
```python
evaluator = AgentEvaluator(graph, config=EvalConfig.default())
report = await evaluator.evaluate("tests/my_tests.json")
print(report.format_summary())
```

**After (Proposed)**:
```python
# evaluate.py

async def evaluate(
    graph,
    test_cases: str | list[dict],  # Path to JSON or list of test dicts
    criteria: list[callable] | str = "default",  # Functions or preset name
    parallel: bool = False,
    verbose: bool = False,
) -> dict:
    """Evaluate an agent graph against test cases.
    
    Args:
        graph: Compiled agent graph to evaluate
        test_cases: Path to JSON file or list of test case dicts
        criteria: List of criterion functions, or preset name ("default", "strict", "relaxed")
        parallel: Whether to run tests in parallel
        verbose: Whether to print progress
    
    Returns:
        {
            "total": int,
            "passed": int,
            "failed": int,
            "pass_rate": float,
            "results": list[dict],  # Individual test results
        }
    
    Example:
        ```python
        # Simple usage
        results = await evaluate(graph, "tests/my_tests.json")
        print(f"Pass rate: {results['pass_rate']:.1%}")
        
        # Custom criteria
        results = await evaluate(
            graph,
            "tests/my_tests.json",
            criteria=[trajectory_match, response_similarity],
        )
        
        # With custom function
        def my_criterion(actual, expected):
            return {"passed": True, "score": 1.0}
        
        results = await evaluate(
            graph,
            test_cases=[{"input": "...", "expected": "..."}],
            criteria=[my_criterion],
        )
        ```
    """
    # Load test cases
    if isinstance(test_cases, str):
        test_cases = _load_json(test_cases)
    
    # Get criteria functions
    if isinstance(criteria, str):
        criteria = _get_preset_criteria(criteria)
    
    # Run evaluations
    results = []
    for test_case in test_cases:
        if verbose:
            print(f"Running test: {test_case.get('name', '...')}")
        
        result = await _evaluate_case(
            graph,
            test_case,
            criteria,
            verbose=verbose,
        )
        results.append(result)
    
    # Compute summary
    passed = sum(1 for r in results if r["passed"])
    
    return {
        "total": len(results),
        "passed": passed,
        "failed": len(results) - passed,
        "pass_rate": passed / len(results) if results else 0.0,
        "results": results,
    }


async def _evaluate_case(
    graph,
    test_case: dict,
    criteria: list[callable],
    verbose: bool = False,
) -> dict:
    """Evaluate a single test case."""
    # Extract test data
    input_data = test_case["input"]
    expected = test_case.get("expected", {})
    
    # Run graph with trajectory collection
    from agentflow.evaluation.collectors import TrajectoryCollector
    
    collector = TrajectoryCollector()
    
    result = await graph.ainvoke(
        input_data,
        config={"callbacks": [collector.on_event]},
    )
    
    # Extract actual outputs
    actual_response = _extract_response(result)
    actual_trajectory = collector.tool_calls
    
    # Run all criteria
    criterion_results = []
    for criterion_fn in criteria:
        try:
            # Call criterion function
            cr_result = await _call_criterion(
                criterion_fn,
                actual={
                    "response": actual_response,
                    "trajectory": actual_trajectory,
                    "collector": collector,
                },
                expected=expected,
            )
            criterion_results.append(cr_result)
        except Exception as e:
            criterion_results.append({
                "passed": False,
                "score": 0.0,
                "error": str(e),
            })
    
    # Combine results
    overall_passed = all(r["passed"] for r in criterion_results)
    avg_score = sum(r["score"] for r in criterion_results) / len(criterion_results)
    
    return {
        "test_id": test_case.get("id", "unknown"),
        "name": test_case.get("name"),
        "passed": overall_passed,
        "score": avg_score,
        "criteria_results": criterion_results,
        "actual_response": actual_response,
        "actual_trajectory": actual_trajectory,
    }


def _get_preset_criteria(preset: str) -> list[callable]:
    """Get preset criterion functions."""
    if preset == "default":
        return [
            lambda a, e: trajectory_match(
                a["trajectory"],
                e.get("trajectory", []),
                match_type="exact",
            ),
            lambda a, e: response_similarity(
                a["response"],
                e.get("response", ""),
                method="rouge1",
            ),
        ]
    elif preset == "strict":
        return [
            lambda a, e: trajectory_match(
                a["trajectory"],
                e.get("trajectory", []),
                match_type="exact",
                check_args=True,
            ),
            lambda a, e: response_similarity(
                a["response"],
                e.get("response", ""),
                method="rouge1",
            ),
            lambda a, e: llm_judge(
                a["response"],
                e.get("response", ""),
                e.get("question", ""),
            ),
        ]
    # ... other presets
    
    return []
```

**Benefits**:
- âœ… Simple function call
- âœ… Easy to customize
- âœ… Still powerful for complex cases
- âœ… Clear escape hatches

---

## ðŸ“‰ Complexity Reduction Comparison

### Lines of Code

| Component | Before | After | Reduction |
|-----------|--------|-------|-----------|
| **Criteria** | ~800 lines (5 files) | ~300 lines (1 file) | 62% |
| **Evaluator** | ~560 lines | ~200 lines | 64% |
| **Configuration** | ~286 lines | ~50 lines (dict validation) | 82% |
| **Results** | ~351 lines | ~100 lines | 71% |
| **Total Core** | ~2,000 lines | ~650 lines | **67% reduction** |

### User Code Complexity

**Before (Current)**:
```python
# User needs to understand:
# - BaseCriterion, SyncCriterion
# - EvalConfig, CriterionConfig
# - EvalReport, EvalCaseResult, CriterionResult
# - AgentEvaluator class

from agentflow.evaluation import (
    AgentEvaluator,
    EvalConfig,
    CriterionConfig,
    MatchType,
)

config = EvalConfig(
    criteria={
        "trajectory": CriterionConfig.trajectory(
            threshold=1.0,
            match_type=MatchType.EXACT,
        ),
    }
)

evaluator = AgentEvaluator(graph, config=config)
report = await evaluator.evaluate("tests/my_tests.json")

# To create custom criterion - need to subclass BaseCriterion
class MyCustomCriterion(BaseCriterion):
    name = "my_criterion"
    
    def __init__(self, config: CriterionConfig | None = None):
        super().__init__(config)
    
    async def evaluate(
        self,
        actual: TrajectoryCollector,
        expected: EvalCase,
    ) -> CriterionResult:
        # Custom logic
        return CriterionResult.success(...)
```

**After (Proposed)**:
```python
# User just needs to understand:
# - evaluate() function
# - Dict-based results
# - Custom criteria are just functions

from agentflow.evaluation import evaluate

# Simple usage
results = await evaluate(graph, "tests/my_tests.json")
print(f"Passed: {results['passed']}/{results['total']}")

# Custom criterion - just a function!
def my_criterion(actual, expected):
    # Your logic here
    score = 1.0 if actual["response"] == expected["response"] else 0.0
    return {
        "passed": score >= 0.8,
        "score": score,
        "details": {},
    }

results = await evaluate(
    graph,
    "tests/my_tests.json",
    criteria=[my_criterion],
)
```

---

## ðŸš€ Migration Strategy

### Phase 1: Create New API (Weeks 1-2)

1. Create `agentflow/evaluation/v2/` directory
2. Implement simplified API:
   - `evaluate.py` - Main evaluation function
   - `criteria.py` - Function-based criteria
   - `models.py` - Simple data models
3. Keep old API in place (backward compatibility)

### Phase 2: Add Deprecation Warnings (Week 3)

1. Add warnings to old API:
   ```python
   import warnings
   
   class AgentEvaluator:
       def __init__(self, ...):
           warnings.warn(
               "AgentEvaluator is deprecated. Use evaluate() instead. "
               "See: docs/evaluation/migration.md",
               DeprecationWarning,
               stacklevel=2,
           )
   ```

2. Update documentation with migration guide

### Phase 3: Update Examples (Week 4)

1. Update all examples to use new API
2. Add migration examples showing before/after

### Phase 4: Remove Old API (Version 0.7.0)

1. Remove deprecated classes
2. Move v2 to main location
3. Clean up imports

---

## ðŸ“š Documentation Strategy

### Quick Start Guide

```python
"""
Quick Start: Agent Evaluation
==============================

Basic Usage
-----------

from agentflow.evaluation import evaluate

# Evaluate with defaults
results = await evaluate(graph, "tests/my_tests.json")

print(f"Pass rate: {results['pass_rate']:.1%}")
for result in results['results']:
    status = "âœ“" if result['passed'] else "âœ—"
    print(f"{status} {result['name']}: {result['score']:.2f}")


Custom Criteria
---------------

def contains_keywords(actual, expected):
    keywords = expected.get("keywords", [])
    found = sum(1 for kw in keywords if kw in actual["response"])
    score = found / len(keywords) if keywords else 1.0
    return {"passed": score >= 0.8, "score": score}

results = await evaluate(
    graph,
    test_cases=[
        {
            "input": {"question": "What's the weather?"},
            "expected": {"keywords": ["temperature", "sunny"]},
        }
    ],
    criteria=[contains_keywords],
)


Composing Criteria
------------------

from agentflow.evaluation import (
    trajectory_match,
    response_similarity,
    combine_criteria,
)

def my_combined_criterion(actual, expected):
    traj_result = trajectory_match(
        actual["trajectory"],
        expected["trajectory"],
    )
    resp_result = response_similarity(
        actual["response"],
        expected["response"],
    )
    return combine_criteria([traj_result, resp_result], logic="all")

results = await evaluate(graph, tests, criteria=[my_combined_criterion])
"""
```

---

## âœ… Benefits Summary

### For AgentFlow Team

1. **Less Code to Maintain**
   - 67% reduction in core evaluation code
   - Fewer abstractions to document
   - Easier to debug and extend

2. **Simpler Testing**
   - Pure functions are easier to test
   - No complex mock hierarchies

3. **Better Focus**
   - Focus on evaluation logic, not class hierarchies
   - More time for features, less for boilerplate

### For Users

1. **Easier Learning Curve**
   - Functions instead of classes
   - Dict-based config instead of nested Pydantic models
   - Clear examples

2. **Easier Extension**
   - Custom criteria = just write a function
   - No need to understand inheritance
   - Compose functions naturally

3. **More Flexible**
   - Can still use classes if needed
   - Multiple ways to achieve same goal
   - Clear escape hatches

---

## ðŸŽ¯ Success Metrics

### Code Metrics
- [ ] Reduce evaluation core from ~2,000 to ~650 lines (67% reduction)
- [ ] Reduce user code for simple case from 15 lines to 3 lines (80% reduction)
- [ ] Zero breaking changes (maintain backward compatibility during transition)

### User Experience
- [ ] Custom criterion creation time: < 5 minutes (vs 20+ minutes now)
- [ ] New user onboarding time: < 10 minutes (vs 30+ minutes now)
- [ ] Common evaluation task LOC: < 5 lines (vs 15+ lines now)

### Documentation
- [ ] Quick start guide: < 100 lines (vs 300+ lines now)
- [ ] Number of concepts to learn: < 5 (vs 15+ now)

---

## ðŸ” Comparison with Other Libraries

### pytest (Testing Framework)

**Why pytest is simple**:
```python
# Just write functions
def test_my_function():
    result = my_function(5)
    assert result == 10
```

**Apply to AgentFlow**:
```python
# Just write functions
def my_evaluation(actual, expected):
    return {"passed": actual == expected, "score": 1.0}
```

### FastAPI (Web Framework)

**Why FastAPI is simple**:
```python
# Just write functions with type hints
@app.get("/")
def read_root():
    return {"message": "Hello"}
```

**Apply to AgentFlow**:
```python
# Just write functions
def my_criterion(actual: dict, expected: dict) -> dict:
    return {"passed": True, "score": 1.0}
```

---

## âš ï¸ Potential Concerns

### Concern 1: "We lose type safety"

**Response**: 
- Still use TypedDict or Pydantic for validation
- Type hints on functions
- Runtime validation where needed

```python
from typing import TypedDict

class CriterionResult(TypedDict):
    passed: bool
    score: float
    details: dict

def my_criterion(actual: dict, expected: dict) -> CriterionResult:
    return {"passed": True, "score": 1.0, "details": {}}
```

### Concern 2: "Functions can't maintain state"

**Response**:
- Use closures for stateful criteria
- Can still use classes when needed (escape hatch)

```python
def create_llm_judge_criterion(model: str = "gpt-4o-mini"):
    """Factory function that creates a criterion with state."""
    from openai import AsyncOpenAI
    
    client = AsyncOpenAI()  # State maintained in closure
    
    async def llm_judge(actual: dict, expected: dict) -> dict:
        # Can use client here
        response = await client.chat.completions.create(model=model, ...)
        return {"passed": True, "score": 1.0}
    
    return llm_judge

# Usage
judge = create_llm_judge_criterion(model="gpt-4")
results = await evaluate(graph, tests, criteria=[judge])
```

### Concern 3: "Loses IDE autocomplete"

**Response**:
- Type hints provide autocomplete
- Clear function signatures
- Better than navigating class hierarchies

---

## ðŸ“‹ Action Items

### Immediate (This Week)
1. âœ… **Review this plan** with team
2. â¬œ **Get approval** on direction
3. â¬œ **Create prototype** of new API

### Short Term (Weeks 1-4)
1. â¬œ **Implement new API** in v2 directory
2. â¬œ **Write migration guide**
3. â¬œ **Update examples**
4. â¬œ **Add deprecation warnings**

### Long Term (Months 2-3)
1. â¬œ **Remove old API** in v0.7.0
2. â¬œ **Update all documentation**
3. â¬œ **Gather user feedback**
4. â¬œ **Refine based on usage**

---

## ðŸ“– Related Documents

- [LLM SDK Migration Plan](./LLM_SDK_MIGRATION_PLAN.md) - Similar abstraction reduction approach
- [LLM Library Comparison](./LLM_LIBRARY_COMPARISON.md) - Minimal wrapper pattern explained

---

## ðŸŽ¬ Conclusion

**The Problem**: Over-abstraction in evaluation module makes it hard to use and maintain.

**The Solution**: Apply minimal wrapper pattern - functions over classes, composition over inheritance.

**The Result**: 
- 67% less code to maintain
- 80% less user code for common cases
- Easier to learn, extend, and debug
- Still powerful for complex use cases

**Recommendation**: Proceed with implementation, maintain backward compatibility during transition.

---

**Questions? Contact the AgentFlow team.**

**Last Updated**: January 8, 2026  
**Status**: Awaiting Approval  
**Next Step**: Team review and feedback
